<!doctype html>
<html>
<head>
<meta charset="UTF-8">
<title>daniela massiceti // stereosonic vision</title>

<link href="{{url_for('static', filename='main.css')}}" rel="stylesheet" type="text/css" media="screen"/>
<link rel="stylesheet" type="text/css" href="{{ url_for('static', filename='twitter-bootstrap.css')}}" />
<link rel="shortcut icon" href="{{ url_for('static', filename='tabby.ico')}}" >
<link rel="icon" href="{{ url_for('static', filename='tabby.ico')}}">

<link rel="stylesheet" type="text/css" href="//fonts.googleapis.com/css?family=Comfortaa" />

<meta name="keywords" content="daniela massiceti, university of oxford, phd student, phil torr, torr vision group, computer vision, oxford">
<meta name="description" content="daniela massiceti // computer vision PhD student" />
<meta name="p:domain_verify" content="15f149f4e12ce0bf04ad541a6e0d8a62"/>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-49520497-2', 'auto');
  ga('send', 'pageview');
</script>

{% if scroll %}
<script> document.location.hash = '#' + '{{ scroll }}'; </script>
{% endif %}
</head>

<body>
<div id="wrapper">
	<header id="top">
    <a href="http://za.linkedin.com/pub/daniela-massiceti/66/422/425" target=_blank><img id="linkedin" src="{{ url_for('static', filename='socialmediaicons/linkedin.png')}}" width="30" height="30" title="linkedin" alt="linkedin"/></a>
    <a href="mailto:daniela@robots.ox.ac.uk" target="_top"><img id="mail" src="{{ url_for('static', filename='socialmediaicons/mail_newcolour.png')}}" width="30" height="30" title="mail: daniela@robots.ox.ac.uk" alt="mail"/></a>
    <a href="https://www.twitter.com/DanniMassi/" target=_blank><img id="twitter" src="{{ url_for('static', filename='socialmediaicons/twitter.png')}}" width="30" height="30" title="twitter: @dannimassi" alt="twitter"/></a>
  </header>
   
  <div id="headerimg">
    <a href="{{ url_for('home') }}">
      <div class = "h">Daniela Massiceti</div>
    </a>
	<div class ="subh">D.Phil Student in Computer Vision, University of Oxford </div>
  </div>
 	<hr id="tophr"> 
	    <nav class="navigationbar" id="extendedbar">
    		<ul>
          <li><a class="normal" href="{{ url_for('home') }}" id=" ">home</a></li>
          <li><a class="normal" href="{{ url_for('about') }}" id=" ">about</a></li>
          <li><a class="normal" href="{{ url_for('research') }}" id=" ">research</a> <a class="normal" href=" {{ url_for('.home') }}" id="currentPage">&middot sonic-vision</a></li>
          <li><a href="{{ url_for('static', filename='')}}{{ cv }}" target=_blank id=" ">cv</a></li>
    		</ul>
      </nav>
  <hr id="demo">
  <div id="mainbody">
    <div id="titlebox">
      <h1>Stereosonic Vision: exploring visual-to-auditory sensory substitution mappings<br>in an immersive virtual reality navigation paradigm</h1>
      <h2>Daniela Massiceti, Stephen L. Hicks, Joram J. van Rheede</h2><br>
    </div>

    <div class="abstract">	
			Vision predominantly underlies our ability to successfully navigate spaces, and the loss of vision can result in devastating consequences for independent navigation and
      mobility. The recent development of devices that can extract 3D spatial information from visual scenes has opened up the possibility of using such mobility-relevant
      information to assist blind and partially sighted people to navigate by presenting this information through modalities other than vision, for example with Sensory
      Substitution Devices (SSDs). In this work, we present two new methods for encoding visual scenes using stereo audio: simulated echolocation and proximity-dependent volume
      modulation, both of which have the potential to be integrated into these SSDs. We call this <i>stereosonic vision</i>. We implemented both methods in a virtual reality (VR) environment and tested them using a 3D motion-tracking device that allowed participants to physically walk through virtual mobility scenarios. The testing paradigm allowed us to capture the dynamics of real mobility behaviour as participants navigated virtual spaces. Participants completed two tasks: navigation within a maze, and obstacle avoidance within a corridor. In both tasks, participants were able to navigate using only audio information. Task completion time, velocity and number of collisions were used as indicators of navigational efficiency, with additional metrics exploring detailed behavioural aspects of their navigation. While speed of navigation using sight outperforms navigation with sound by 65%, a clear learning effect is observed with participants improving their sound-based navigation by an average 21% in completion time after just 6 trials. Importantly, participants were able to achieve this with minimal instructions and a very short training period. Ultimately this work explores the intuitiveness of two vision-to-audio mappings in conveying a sense of 3D spatial awareness for successful navigation, and motivates the exploration of further such mappings with the goal of enabling independent mobility for visually-impaired individuals.				
    </div>
    <img class="abstractimg" style="width: 50%" src="{{ url_for('.static', filename='abstracthook.svg') }}" alt=""/>

	  <hr id="tophr2">
    <nav class="navigationbar">
      <ul>
        <li><a class="normal" href="{{ url_for('stereosonic_vision.home')}}" id="currentPage">abstract</a></li>
        <li>paper (coming)</li>
        <li><a class="normal" href="{{ url_for('.static', filename='stereosonic_vision_data.zip')}}" id=" ">data (212MB)</a></li>
      </ul>
    </nav>
    <hr id="bottomhr2">

  	<div id="footerwrapper">
		  <div id="email"> daniela_at_robots.ox.ac.uk </div>
      <img src="{{ url_for('static', filename='robotgirl.png') }}" alt=""/>
	  </div>
  </div>
</div>
</body>
</html>
